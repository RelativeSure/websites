import { Bot, Check, Copy, Download, ExternalLink, Plus, X } from "lucide-react";
import { useState } from "react";
import { Button } from "@/components/ui/button";
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@/components/ui/card";
import { Input } from "@/components/ui/input";
import { Label } from "@/components/ui/label";
import { Textarea } from "@/components/ui/textarea";

interface Rule {
  userAgent: string;
  allow: string[];
  disallow: string[];
}

export default function RobotsTxtPage() {
  const [rules, setRules] = useState<Rule[]>([{ userAgent: "*", allow: [], disallow: [] }]);
  const [sitemap, setSitemap] = useState("");
  const [crawlDelay, setCrawlDelay] = useState("");
  const [host, setHost] = useState("");
  const [output, setOutput] = useState("");
  const [copied, setCopied] = useState(false);

  const addRule = () => {
    setRules([...rules, { userAgent: "", allow: [], disallow: [] }]);
  };

  const removeRule = (index: number) => {
    setRules(rules.filter((_, i) => i !== index));
  };

  const updateUserAgent = (index: number, value: string) => {
    const newRules = [...rules];
    newRules[index].userAgent = value;
    setRules(newRules);
  };

  const updatePaths = (index: number, type: "allow" | "disallow", value: string) => {
    const newRules = [...rules];
    newRules[index][type] = value
      .split("\n")
      .map((p) => p.trim())
      .filter((p) => p);
    setRules(newRules);
  };

  const generateRobotsTxt = () => {
    let txt = "# robots.txt generated by DevTools\n\n";

    rules.forEach((rule) => {
      if (!rule.userAgent) return;

      txt += `User-agent: ${rule.userAgent}\n`;

      rule.disallow.forEach((path) => {
        txt += `Disallow: ${path}\n`;
      });

      rule.allow.forEach((path) => {
        txt += `Allow: ${path}\n`;
      });

      if (crawlDelay && rule.userAgent === "*") {
        txt += `Crawl-delay: ${crawlDelay}\n`;
      }

      txt += "\n";
    });

    if (sitemap) {
      sitemap.split("\n").forEach((sm) => {
        const trimmed = sm.trim();
        if (trimmed) txt += `Sitemap: ${trimmed}\n`;
      });
    }

    if (host) {
      txt += `\nHost: ${host}\n`;
    }

    setOutput(txt);
  };

  const loadTemplate = (template: string) => {
    switch (template) {
      case "basic":
        setRules([{ userAgent: "*", allow: [], disallow: ["/admin/", "/private/"] }]);
        break;
      case "allow-all":
        setRules([{ userAgent: "*", allow: [], disallow: [] }]);
        break;
      case "block-all":
        setRules([{ userAgent: "*", allow: [], disallow: ["/"] }]);
        break;
      case "seo-friendly":
        setRules([
          {
            userAgent: "*",
            allow: [],
            disallow: ["/admin/", "/api/", "/temp/", "/*.json$"],
          },
          {
            userAgent: "Googlebot",
            allow: ["/"],
            disallow: ["/admin/"],
          },
        ]);
        setSitemap("https://example.com/sitemap.xml");
        break;
      case "bad-bots":
        setRules([
          {
            userAgent: "*",
            allow: [],
            disallow: [],
          },
          {
            userAgent: "BadBot",
            allow: [],
            disallow: ["/"],
          },
          {
            userAgent: "AnotherBadBot",
            allow: [],
            disallow: ["/"],
          },
        ]);
        break;
    }
  };

  const handleCopy = async () => {
    try {
      await navigator.clipboard.writeText(output);
      setCopied(true);
      setTimeout(() => setCopied(false), 2000);
    } catch (err) {
      console.error("Failed to copy:", err);
    }
  };

  const handleDownload = () => {
    const blob = new Blob([output], { type: "text/plain" });
    const url = URL.createObjectURL(blob);
    const link = document.createElement("a");
    link.href = url;
    link.download = "robots.txt";
    document.body.appendChild(link);
    link.click();
    document.body.removeChild(link);
    URL.revokeObjectURL(url);
  };

  const commonBots = [
    "*",
    "Googlebot",
    "Bingbot",
    "Slurp",
    "DuckDuckBot",
    "Baiduspider",
    "YandexBot",
    "facebookexternalhit",
    "Twitterbot",
  ];

  return (
    <div className="container mx-auto p-6 max-w-6xl">
      <div className="mb-6">
        <h1 className="text-3xl font-bold mb-2">robots.txt Generator</h1>
        <p className="text-muted-foreground">Generate robots.txt file to control crawler access to your website</p>
      </div>

      <div className="grid grid-cols-1 lg:grid-cols-3 gap-6 mb-6">
        <Card className="lg:col-span-2">
          <CardHeader>
            <div className="flex items-center justify-between">
              <div>
                <CardTitle className="flex items-center gap-2">
                  <Bot className="h-5 w-5" />
                  Rules Configuration
                </CardTitle>
                <CardDescription>Define rules for different user agents</CardDescription>
              </div>
              <Button onClick={addRule} size="sm">
                <Plus className="h-4 w-4 mr-1" />
                Add Rule
              </Button>
            </div>
          </CardHeader>
          <CardContent className="space-y-6">
            {rules.map((rule, index) => (
              <div key={index} className="border rounded-lg p-4 space-y-3">
                <div className="flex items-center justify-between">
                  <Label>Rule {index + 1}</Label>
                  {rules.length > 1 && (
                    <Button variant="ghost" size="sm" onClick={() => removeRule(index)}>
                      <X className="h-4 w-4" />
                    </Button>
                  )}
                </div>

                <div className="space-y-2">
                  <Label htmlFor={`agent-${index}`}>User-agent</Label>
                  <Input
                    id={`agent-${index}`}
                    value={rule.userAgent}
                    onChange={(e) => updateUserAgent(index, e.target.value)}
                    placeholder="* or Googlebot"
                    list={`bots-${index}`}
                  />
                  <datalist id={`bots-${index}`}>
                    {commonBots.map((bot) => (
                      <option key={bot} value={bot} />
                    ))}
                  </datalist>
                </div>

                <div className="space-y-2">
                  <Label htmlFor={`disallow-${index}`}>Disallow (one path per line)</Label>
                  <Textarea
                    id={`disallow-${index}`}
                    value={rule.disallow.join("\n")}
                    onChange={(e) => updatePaths(index, "disallow", e.target.value)}
                    placeholder="/admin/&#10;/private/&#10;/*.json$"
                    className="font-mono text-sm"
                    rows={3}
                  />
                </div>

                <div className="space-y-2">
                  <Label htmlFor={`allow-${index}`}>Allow (one path per line)</Label>
                  <Textarea
                    id={`allow-${index}`}
                    value={rule.allow.join("\n")}
                    onChange={(e) => updatePaths(index, "allow", e.target.value)}
                    placeholder="/public/&#10;/assets/"
                    className="font-mono text-sm"
                    rows={2}
                  />
                </div>
              </div>
            ))}
          </CardContent>
        </Card>

        <Card>
          <CardHeader>
            <CardTitle>Global Settings</CardTitle>
            <CardDescription>Optional configuration</CardDescription>
          </CardHeader>
          <CardContent className="space-y-4">
            <div className="space-y-2">
              <Label htmlFor="sitemap">Sitemap URL(s)</Label>
              <Textarea
                id="sitemap"
                value={sitemap}
                onChange={(e) => setSitemap(e.target.value)}
                placeholder="https://example.com/sitemap.xml"
                className="font-mono text-sm"
                rows={2}
              />
              <div className="text-xs text-muted-foreground">One URL per line</div>
            </div>

            <div className="space-y-2">
              <Label htmlFor="crawl-delay">Crawl-delay (seconds)</Label>
              <Input
                id="crawl-delay"
                type="number"
                value={crawlDelay}
                onChange={(e) => setCrawlDelay(e.target.value)}
                placeholder="10"
              />
              <div className="text-xs text-muted-foreground">Not supported by all crawlers</div>
            </div>

            <div className="space-y-2">
              <Label htmlFor="host">Host (preferred domain)</Label>
              <Input
                id="host"
                value={host}
                onChange={(e) => setHost(e.target.value)}
                placeholder="https://www.example.com"
              />
              <div className="text-xs text-muted-foreground">Yandex specific</div>
            </div>
          </CardContent>
        </Card>
      </div>

      <Card className="mb-6">
        <CardHeader>
          <CardTitle>Templates</CardTitle>
          <CardDescription>Quick start with common configurations</CardDescription>
        </CardHeader>
        <CardContent>
          <div className="flex flex-wrap gap-2">
            <Button variant="outline" size="sm" onClick={() => loadTemplate("allow-all")}>
              Allow All
            </Button>
            <Button variant="outline" size="sm" onClick={() => loadTemplate("block-all")}>
              Block All
            </Button>
            <Button variant="outline" size="sm" onClick={() => loadTemplate("basic")}>
              Basic Protection
            </Button>
            <Button variant="outline" size="sm" onClick={() => loadTemplate("seo-friendly")}>
              SEO Friendly
            </Button>
            <Button variant="outline" size="sm" onClick={() => loadTemplate("bad-bots")}>
              Block Bad Bots
            </Button>
          </div>
        </CardContent>
      </Card>

      <div className="mb-6">
        <Button onClick={generateRobotsTxt} size="lg" className="w-full lg:w-auto">
          <Bot className="h-4 w-4 mr-2" />
          Generate robots.txt
        </Button>
      </div>

      {output && (
        <Card className="mb-6">
          <CardHeader>
            <div className="flex items-center justify-between">
              <div>
                <CardTitle>Generated robots.txt</CardTitle>
                <CardDescription>Place this file at the root of your website</CardDescription>
              </div>
              <div className="flex gap-2">
                <Button variant="ghost" size="sm" onClick={handleCopy}>
                  {copied ? (
                    <>
                      <Check className="h-4 w-4 mr-1" />
                      Copied
                    </>
                  ) : (
                    <>
                      <Copy className="h-4 w-4 mr-1" />
                      Copy
                    </>
                  )}
                </Button>
                <Button variant="ghost" size="sm" onClick={handleDownload}>
                  <Download className="h-4 w-4 mr-1" />
                  Download
                </Button>
              </div>
            </div>
          </CardHeader>
          <CardContent>
            <Textarea value={output} readOnly className="font-mono text-sm min-h-[300px] bg-muted" />
          </CardContent>
        </Card>
      )}

      <Card>
        <CardHeader>
          <CardTitle>About robots.txt</CardTitle>
        </CardHeader>
        <CardContent className="text-sm text-muted-foreground space-y-3">
          <p>
            The robots.txt file tells search engine crawlers which URLs they can access on your site. It must be placed
            at the root of your website (e.g., https://example.com/robots.txt).
          </p>
          <div>
            <strong>Common directives:</strong>
            <ul className="list-disc list-inside ml-4 mt-1 space-y-1">
              <li>
                <strong>User-agent:</strong> Specifies which crawler the rules apply to
              </li>
              <li>
                <strong>Disallow:</strong> URLs that crawlers should not access
              </li>
              <li>
                <strong>Allow:</strong> Override disallow for specific paths
              </li>
              <li>
                <strong>Sitemap:</strong> Location of your sitemap(s)
              </li>
              <li>
                <strong>Crawl-delay:</strong> Time to wait between requests (in seconds)
              </li>
            </ul>
          </div>
          <p>
            <strong>Important:</strong> robots.txt is not a security mechanism. Malicious bots may ignore it. Use proper
            authentication for sensitive content.
          </p>
          <div className="flex items-center gap-2 pt-2">
            <ExternalLink className="h-4 w-4" />
            <a
              href="https://developers.google.com/search/docs/crawling-indexing/robots/intro"
              target="_blank"
              rel="noopener noreferrer"
              className="underline hover:text-primary"
            >
              Learn more about robots.txt
            </a>
          </div>
        </CardContent>
      </Card>
    </div>
  );
}
